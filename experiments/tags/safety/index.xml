<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Safety on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/safety/</link>
    <description>Recent content in Safety on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/safety/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reward Hacking in Reinforcement Learning</title>
      <link>https://lilianweng.github.io/posts/2024-11-28-reward-hacking/</link>
      <pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2024-11-28-reward-hacking/</guid>
      <description>&lt;p&gt;Reward hacking occurs when a &lt;a href=&#34;(https://lilianweng.github.io/posts/2018-02-19-rl-overview/)&#34;&gt;reinforcement learning (RL)&lt;/a&gt; agent &lt;a href=&#34;https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#exploitation-vs-exploration&#34;&gt;exploits&lt;/a&gt; flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function.&lt;/p&gt;
&lt;p&gt;With the rise of &lt;a href=&#34;https://lilianweng.github.io/posts/2019-01-31-lm/&#34;&gt;language models&lt;/a&gt; generalizing to a broad spectrum of tasks and RLHF becomes a de facto method for alignment training, reward hacking in RL training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user&amp;rsquo;s preference, are pretty concerning and are likely one of the major blockers for real-world deployment of more autonomous use cases of AI models.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Extrinsic Hallucinations in LLMs</title>
      <link>https://lilianweng.github.io/posts/2024-07-07-hallucination/</link>
      <pubDate>Sun, 07 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2024-07-07-hallucination/</guid>
      <description>&lt;p&gt;Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and &lt;strong&gt;not grounded&lt;/strong&gt; by either the provided context or world knowledge.&lt;/p&gt;
&lt;p&gt;There are two types of hallucination:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In-context hallucination: The model output should be consistent with the source content in context.&lt;/li&gt;
&lt;li&gt;Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Adversarial Attacks on LLMs</title>
      <link>https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/</link>
      <pubDate>Wed, 25 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/</guid>
      <description>&lt;p&gt;The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via &lt;a href=&#34;https://openai.com/research/learning-to-summarize-with-human-feedback&#34;&gt;RLHF&lt;/a&gt;). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.&lt;/p&gt;
&lt;p&gt;A large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on &lt;a href=&#34;https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/&#34;&gt;Controllable Text Generation&lt;/a&gt; is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Reducing Toxicity in Language Models</title>
      <link>https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/</link>
      <pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/</guid>
      <description>&lt;!-- Toxicity prevents us from safely deploying powerful pretrained language models for real-world applications. To reduce toxicity in language models, in this post, we will delve into three aspects of the problem: training dataset collection, toxic content detection and model detoxification. --&gt;
&lt;p&gt;Large pretrained &lt;a href=&#34;https://lilianweng.github.io/posts/2019-01-31-lm/&#34;&gt;language models&lt;/a&gt; are trained over a sizable collection of online data. They unavoidably acquire certain toxic behavior and biases from the Internet. Pretrained language models are very powerful and have shown great success in many NLP tasks. However, to safely deploy them for practical real-world applications demands a strong safety control over the model generation process.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
