<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Math-Heavy on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/math-heavy/</link>
    <description>Recent content in Math-Heavy on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 11 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/math-heavy/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>What are Diffusion Models?</title>
      <link>https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</link>
      <pubDate>Sun, 11 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</guid>
      <description>&lt;!-- Diffusion models are a new type of generative models that are flexible enough to learn any arbitrarily complex data distribution while tractable to analytically evaluate the distribution. It has been shown recently that diffusion models can generate high-quality images and the performance is competitive to SOTA GAN. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2021-09-19: Highly recommend this blog post on &lt;a href=&#34;https://yang-song.github.io/blog/2021/score/&#34;&gt;score-based generative modeling&lt;/a&gt; by Yang Song (author of several key papers in the references)].&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2022-08-27: Added &lt;a href=&#34;#classifier-free-guidance&#34;&gt;classifier-free guidance&lt;/a&gt;, &lt;a href=&#34;#glide&#34;&gt;GLIDE&lt;/a&gt;, &lt;a href=&#34;#unclip&#34;&gt;unCLIP&lt;/a&gt; and &lt;a href=&#34;#imagen&#34;&gt;Imagen&lt;/a&gt;.&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2022-08-31: Added &lt;a href=&#34;#ldm&#34;&gt;latent diffusion model&lt;/a&gt;.&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2024-04-13: Added &lt;a href=&#34;#prog-distll&#34;&gt;progressive distillation&lt;/a&gt;, &lt;a href=&#34;#consistency&#34;&gt;consistency models&lt;/a&gt;, and the &lt;a href=&#34;#model-architecture&#34;&gt;Model Architecture section&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Flow-based Deep Generative Models</title>
      <link>https://lilianweng.github.io/posts/2018-10-13-flow-models/</link>
      <pubDate>Sat, 13 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-10-13-flow-models/</guid>
      <description>&lt;!-- In this post, we are looking into the third type of generative models: flow-based generative models. Different from GAN and VAE, they explicitly learn the probability density function of the input data. --&gt;
&lt;p&gt;So far, I&amp;rsquo;ve written about two types of generative models, &lt;a href=&#34;https://lilianweng.github.io/posts/2017-08-20-gan/&#34;&gt;GAN&lt;/a&gt; and &lt;a href=&#34;https://lilianweng.github.io/posts/2018-08-12-vae/&#34;&gt;VAE&lt;/a&gt;. Neither of them explicitly learns the probability density function of real data, $p(\mathbf{x})$ (where $\mathbf{x} \in \mathcal{D}$) &amp;mdash; because it is really hard! Taking the generative model with latent variables as an example, $p(\mathbf{x}) = \int p(\mathbf{x}\vert\mathbf{z})p(\mathbf{z})d\mathbf{z}$ can hardly be calculated as it is intractable to go through all possible values of the latent code $\mathbf{z}$.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Policy Gradient Algorithms</title>
      <link>https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</link>
      <pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</guid>
      <description>&lt;!-- Abstract: In this post, we are going to look deep into policy gradient, why it works, and many new policy gradient algorithms proposed in recent years: vanilla policy gradient, actor-critic, off-policy actor-critic, A3C, A2C, DPG, DDPG, D4PG, MADDPG, TRPO, PPO, ACER, ACTKR, SAC, TD3 &amp; SVPG. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2018-06-30: add two new policy gradient methods, &lt;a href=&#34;#sac&#34;&gt;SAC&lt;/a&gt; and &lt;a href=&#34;#d4pg&#34;&gt;D4PG&lt;/a&gt;.]&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2018-09-30: add a new policy gradient method, &lt;a href=&#34;#td3&#34;&gt;TD3&lt;/a&gt;.]&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2019-02-09: add &lt;a href=&#34;#sac-with-automatically-adjusted-temperature&#34;&gt;SAC with automatically adjusted temperature&lt;/a&gt;].&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2019-06-26: Thanks to Chanseok, we have a version of this post in &lt;a href=&#34;https://talkingaboutme.tistory.com/entry/RL-Policy-Gradient-Algorithms&#34;&gt;Korean&lt;/a&gt;].&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2019-09-12: add a new policy gradient method &lt;a href=&#34;#svpg&#34;&gt;SVPG&lt;/a&gt;.]&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2019-12-22: add a new policy gradient method &lt;a href=&#34;#impala&#34;&gt;IMPALA&lt;/a&gt;.]&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-10-15: add a new policy gradient method &lt;a href=&#34;#ppg&#34;&gt;PPG&lt;/a&gt; &amp;amp; some new discussion in &lt;a href=&#34;#ppo&#34;&gt;PPO&lt;/a&gt;.]&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2021-09-19: Thanks to Wenhao &amp;amp; 爱吃猫的鱼, we have this post in &lt;a href=&#34;https://tomaxent.com/2019/04/14/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%96%B9%E6%B3%95/&#34;&gt;Chinese1&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://paperexplained.cn/articles/article/detail/31/&#34;&gt;Chinese2&lt;/a&gt;].&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A (Long) Peek into Reinforcement Learning</title>
      <link>https://lilianweng.github.io/posts/2018-02-19-rl-overview/</link>
      <pubDate>Mon, 19 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-02-19-rl-overview/</guid>
      <description>&lt;!-- In this post, we are gonna briefly go over the field of Reinforcement Learning (RL), from fundamental concepts to classic algorithms. Hopefully, this review is helpful enough so that newbies would not get lost in specialized terms and jargons while starting. [WARNING] This is a long read. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-09-03: Updated the algorithm of &lt;a href=&#34;#sarsa-on-policy-td-control&#34;&gt;SARSA&lt;/a&gt; and &lt;a href=&#34;#q-learning-off-policy-td-control&#34;&gt;Q-learning&lt;/a&gt; so that the difference is more pronounced.&lt;/span&gt;
&lt;br /&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2021-09-19: Thanks to 爱吃猫的鱼, we have this post in &lt;a href=&#34;https://paperexplained.cn/articles/article/detail/33/&#34;&gt;Chinese&lt;/a&gt;].&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Multi-Armed Bandit Problem and Its Solutions</title>
      <link>https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/</link>
      <pubDate>Tue, 23 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/</guid>
      <description>&lt;!-- The multi-armed bandit problem is a class example to demonstrate the exploration versus exploitation dilemma. This post introduces the bandit problem and how to solve it using different exploration strategies. --&gt;
&lt;p&gt;The algorithms are implemented for Bernoulli bandit in &lt;a href=&#34;http://github.com/lilianweng/multi-armed-bandit&#34;&gt;lilianweng/multi-armed-bandit&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;exploitation-vs-exploration&#34;&gt;Exploitation vs Exploration&lt;/h1&gt;
&lt;p&gt;The exploration vs exploitation dilemma exists in many aspects of our life. Say, your favorite restaurant is right around the corner. If you go there every day, you would be confident of what you will get, but miss the chances of discovering an even better option. If you try new places all the time, very likely you are gonna have to eat unpleasant food from time to time. Similarly, online advisors try to balance between the known most attractive ads and the new ads that might be even more successful.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>From GAN to WGAN</title>
      <link>https://lilianweng.github.io/posts/2017-08-20-gan/</link>
      <pubDate>Sun, 20 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2017-08-20-gan/</guid>
      <description>&lt;!-- This post explains the maths behind a generative adversarial network (GAN) model and why it is hard to be trained. Wasserstein GAN is intended to improve GANs&#39; training by adopting a smooth metric for measuring the distance between two probability distributions. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2018-09-30: thanks to Yoonju, we have this post translated in &lt;a href=&#34;https://github.com/yjucho1/articles/blob/master/fromGANtoWGAN/readme.md&#34;&gt;Korean&lt;/a&gt;!]&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2019-04-18: this post is also available on &lt;a href=&#34;https://arxiv.org/abs/1904.08994&#34;&gt;arXiv&lt;/a&gt;.]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1406.2661.pdf&#34;&gt;Generative adversarial network&lt;/a&gt; (GAN) has shown great results in many generative tasks to replicate the real-world rich content such as images, human language, and music. It is inspired by game theory: two models, a generator and a critic, are competing with each other while making each other stronger at the same time. However, it is rather challenging to train a GAN model, as people are facing issues like training instability or failure to converge.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
