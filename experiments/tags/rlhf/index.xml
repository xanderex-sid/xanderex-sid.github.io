<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Rlhf on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/rlhf/</link>
    <description>Recent content in Rlhf on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/rlhf/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reward Hacking in Reinforcement Learning</title>
      <link>https://lilianweng.github.io/posts/2024-11-28-reward-hacking/</link>
      <pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2024-11-28-reward-hacking/</guid>
      <description>&lt;p&gt;Reward hacking occurs when a &lt;a href=&#34;(https://lilianweng.github.io/posts/2018-02-19-rl-overview/)&#34;&gt;reinforcement learning (RL)&lt;/a&gt; agent &lt;a href=&#34;https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#exploitation-vs-exploration&#34;&gt;exploits&lt;/a&gt; flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function.&lt;/p&gt;
&lt;p&gt;With the rise of &lt;a href=&#34;https://lilianweng.github.io/posts/2019-01-31-lm/&#34;&gt;language models&lt;/a&gt; generalizing to a broad spectrum of tasks and RLHF becomes a de facto method for alignment training, reward hacking in RL training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user&amp;rsquo;s preference, are pretty concerning and are likely one of the major blockers for real-world deployment of more autonomous use cases of AI models.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
