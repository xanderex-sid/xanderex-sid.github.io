<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Architecture on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/architecture/</link>
    <description>Recent content in Architecture on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/architecture/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Transformer Family Version 2.0</title>
      <link>https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/</link>
      <pubDate>Fri, 27 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/</guid>
      <description>&lt;p&gt;Many new Transformer architecture improvements have been proposed since my last post on &lt;a href=&#34;https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/&#34;&gt;&lt;ins&gt;&amp;ldquo;The Transformer Family&amp;rdquo;&lt;/ins&gt;&lt;/a&gt; about three years ago. Here I did a big refactoring and enrichment of that 2020 post &amp;mdash; restructure the hierarchy of sections and improve many sections with more recent papers. Version 2.0 is a superset of the old version, about twice the length.&lt;/p&gt;
&lt;h1 id=&#34;notations&#34;&gt;Notations&lt;/h1&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Symbol&lt;/th&gt;
          &lt;th&gt;Meaning&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;$d$&lt;/td&gt;
          &lt;td&gt;The model size / hidden state dimension / positional encoding size.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$h$&lt;/td&gt;
          &lt;td&gt;The number of heads in multi-head attention layer.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$L$&lt;/td&gt;
          &lt;td&gt;The segment length of input sequence.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$N$&lt;/td&gt;
          &lt;td&gt;The total number of attention layers in the model; not considering MoE.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{X} \in \mathbb{R}^{L \times d}$&lt;/td&gt;
          &lt;td&gt;The input sequence where each element has been mapped into an embedding vector of shape $d$, same as the model size.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{W}^k \in \mathbb{R}^{d \times d_k}$&lt;/td&gt;
          &lt;td&gt;The key weight matrix.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{W}^q \in \mathbb{R}^{d \times d_k}$&lt;/td&gt;
          &lt;td&gt;The query weight matrix.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{W}^v \in \mathbb{R}^{d \times d_v}$&lt;/td&gt;
          &lt;td&gt;The value weight matrix. Often we have $d_k = d_v = d$.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{W}^k_i, \mathbf{W}^q_i \in \mathbb{R}^{d \times d_k/h}; \mathbf{W}^v_i \in \mathbb{R}^{d \times d_v/h}$&lt;/td&gt;
          &lt;td&gt;The weight matrices per head.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{W}^o \in \mathbb{R}^{d_v \times d}$&lt;/td&gt;
          &lt;td&gt;The output weight matrix.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{Q} = \mathbf{X}\mathbf{W}^q \in \mathbb{R}^{L \times d_k}$&lt;/td&gt;
          &lt;td&gt;The query embedding inputs.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{K} = \mathbf{X}\mathbf{W}^k \in \mathbb{R}^{L \times d_k}$&lt;/td&gt;
          &lt;td&gt;The key embedding inputs.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{V} = \mathbf{X}\mathbf{W}^v \in \mathbb{R}^{L \times d_v}$&lt;/td&gt;
          &lt;td&gt;The value embedding inputs.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{q}_i, \mathbf{k}_i \in \mathbb{R}^{d_k}, \mathbf{v}_i \in \mathbb{R}^{d_v}$&lt;/td&gt;
          &lt;td&gt;Row vectors in query, key, value matrices, $\mathbf{Q}$, $\mathbf{K}$ and $\mathbf{V}$.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$S_i$&lt;/td&gt;
          &lt;td&gt;A collection of key positions for the $i$-th query $\mathbf{q}_i$ to attend to.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{A} \in \mathbb{R}^{L \times L}$&lt;/td&gt;
          &lt;td&gt;The self-attention matrix between a input sequence of lenght $L$ and itself. $\mathbf{A} = \text{softmax}(\mathbf{Q}\mathbf{K}^\top / \sqrt{d_k})$.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$a_{ij} \in \mathbf{A}$&lt;/td&gt;
          &lt;td&gt;The scalar attention score between query $\mathbf{q}_i$ and key $\mathbf{k}_j$.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{P} \in \mathbb{R}^{L \times d}$&lt;/td&gt;
          &lt;td&gt;position encoding matrix, where the $i$-th row $\mathbf{p}_i$ is the positional encoding for input $\mathbf{x}_i$.&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;transformer-basics&#34;&gt;Transformer Basics&lt;/h1&gt;
&lt;p&gt;The &lt;strong&gt;Transformer&lt;/strong&gt; (which will be referred to as &amp;ldquo;vanilla Transformer&amp;rdquo; to distinguish it from other enhanced versions; &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Vaswani, et al., 2017&lt;/a&gt;) model has an encoder-decoder architecture, as commonly used in many &lt;a href=&#34;https://lilianweng.github.io/posts/2018-06-24-attention/#born-for-translation&#34;&gt;NMT&lt;/a&gt; models. Later simplified Transformer was shown to achieve great performance in language modeling tasks, like in encoder-only &lt;a href=&#34;https://lilianweng.github.io/posts/2019-01-31-lm/#bert&#34;&gt;BERT&lt;/a&gt; or decoder-only &lt;a href=&#34;https://lilianweng.github.io/posts/2019-01-31-lm/#openai-gpt&#34;&gt;GPT&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Large Transformer Model Inference Optimization</title>
      <link>https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</link>
      <pubDate>Tue, 10 Jan 2023 10:00:00 -0700</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</guid>
      <description>&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2023-01-24: add a small section on &lt;a href=&#34;#distillation&#34;&gt;Distillation&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Large transformer models are mainstream nowadays, creating SoTA results for a variety of tasks. They are powerful but very expensive to train and use. The extremely high inference cost, in both time and memory, is a big bottleneck for adopting a powerful transformer for solving real-world tasks at scale.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why is it hard to run inference for large transformer models?&lt;/strong&gt; Besides the increasing size of SoTA models, there are two main factors contributing to the inference challenge (&lt;a href=&#34;https://arxiv.org/abs/2211.05102&#34;&gt;Pope et al. 2022&lt;/a&gt;):&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How to Train Really Large Models on Many GPUs?</title>
      <link>https://lilianweng.github.io/posts/2021-09-25-train-large/</link>
      <pubDate>Fri, 24 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2021-09-25-train-large/</guid>
      <description>&lt;!-- How to train large and deep neural networks is challenging, as it demands a large amount of GPU memory and a long horizon of training time. This post reviews several popular training parallelism paradigms, as well as a variety of model architecture and memory saving designs to make it possible to train very large neural networks across a large number of GPUs. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2022-03-13: add &lt;a href=&#34;#ec&#34;&gt;expert choice routing&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2022-06-10]: &lt;a href=&#34;https://gregbrockman.com/&#34;&gt;Greg&lt;/a&gt; and I wrote a shorted and upgraded version of this post, published on OpenAI Blog: &lt;a href=&#34;https://openai.com/blog/techniques-for-training-large-neural-networks/&#34;&gt;&amp;ldquo;Techniques for Training Large Neural Networks&amp;rdquo;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Neural Architecture Search</title>
      <link>https://lilianweng.github.io/posts/2020-08-06-nas/</link>
      <pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2020-08-06-nas/</guid>
      <description>&lt;!-- Neural Architecture Search (NAS) automates network architecture engineering. It aims to learn a network topology that can achieve best performance on a certain task. By dissecting the methods for NAS into three components: search space, search algorithm and child model evolution strategy, this post reviews many interesting ideas for better, faster and more cost-efficient automatic neural architecture search. --&gt;
&lt;p&gt;Although most popular and successful model architectures are designed by human experts, it doesn&amp;rsquo;t mean we have explored the entire network architecture space and settled down with the best option. We would have a better chance to find the optimal solution if we adopt a systematic and automatic way of learning high-performance model architectures.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Transformer Family</title>
      <link>https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/</link>
      <pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/</guid>
      <description>&lt;!-- Inspired by recent progress on various enhanced versions of Transformer models, this post presents how the vanilla Transformer can be improved for longer-term attention span, less memory and computation consumption, RL task solving, etc. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on &lt;mark&gt;&lt;strong&gt;2023-01-27&lt;/strong&gt;&lt;/mark&gt;: After almost three years, I did a big refactoring update of this post to incorporate a bunch of new Transformer models since 2020. The enhanced version of this post is here: &lt;a href=&#34;https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/&#34;&gt;&lt;mark&gt;&lt;b&gt;The Transformer Family Version 2.0&lt;/b&gt;&lt;/mark&gt;&lt;/a&gt;. Please refer to that post on this topic.]&lt;/span&gt;
&lt;br/&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Generalized Language Models</title>
      <link>https://lilianweng.github.io/posts/2019-01-31-lm/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2019-01-31-lm/</guid>
      <description>&lt;!-- As a follow up of word embedding post, we will discuss the models on learning contextualized word vectors, as well as the new trend in large unsupervised pre-trained language models which have achieved amazing SOTA results on a variety of language tasks. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2019-02-14: add &lt;a href=&#34;#ulmfit&#34;&gt;ULMFiT&lt;/a&gt; and &lt;a href=&#34;#gpt-2&#34;&gt;GPT-2&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-02-29: add &lt;a href=&#34;#albert&#34;&gt;ALBERT&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-10-25: add &lt;a href=&#34;#roberta&#34;&gt;RoBERTa&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-12-13: add &lt;a href=&#34;#t5&#34;&gt;T5&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-12-30: add &lt;a href=&#34;#gpt-3&#34;&gt;GPT-3&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2021-11-13: add &lt;a href=&#34;#xlnet&#34;&gt;XLNet&lt;/a&gt;, &lt;a href=&#34;#bart&#34;&gt;BART&lt;/a&gt; and &lt;a href=&#34;#electra&#34;&gt;ELECTRA&lt;/a&gt;; Also updated the &lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt; section.]&lt;/span&gt;&lt;/p&gt;
&lt;br /&gt;
&lt;figure&gt;
	&lt;img src=&#34;elmo-and-bert.png&#34; style=&#34;width: 60%;&#34;  /&gt;
	&lt;figcaption&gt;I guess they are Elmo &amp; Bert? (Image source: &lt;a href=&#34;https://www.youtube.com/watch?v=l5einDQ-Ttc&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We have seen amazing progress in NLP in 2018. Large-scale pre-trained language modes like &lt;a href=&#34;https://blog.openai.com/language-unsupervised/&#34;&gt;OpenAI GPT&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;BERT&lt;/a&gt; have achieved great performance on a variety of language tasks using generic model architectures. The idea is similar to how ImageNet classification pre-training helps many vision tasks (*). Even better than vision classification pre-training, this simple and powerful approach in NLP does not require labeled data for pre-training, allowing us to experiment with increased training scale, up to our very limit.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Flow-based Deep Generative Models</title>
      <link>https://lilianweng.github.io/posts/2018-10-13-flow-models/</link>
      <pubDate>Sat, 13 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-10-13-flow-models/</guid>
      <description>&lt;!-- In this post, we are looking into the third type of generative models: flow-based generative models. Different from GAN and VAE, they explicitly learn the probability density function of the input data. --&gt;
&lt;p&gt;So far, I&amp;rsquo;ve written about two types of generative models, &lt;a href=&#34;https://lilianweng.github.io/posts/2017-08-20-gan/&#34;&gt;GAN&lt;/a&gt; and &lt;a href=&#34;https://lilianweng.github.io/posts/2018-08-12-vae/&#34;&gt;VAE&lt;/a&gt;. Neither of them explicitly learns the probability density function of real data, $p(\mathbf{x})$ (where $\mathbf{x} \in \mathcal{D}$) &amp;mdash; because it is really hard! Taking the generative model with latent variables as an example, $p(\mathbf{x}) = \int p(\mathbf{x}\vert\mathbf{z})p(\mathbf{z})d\mathbf{z}$ can hardly be calculated as it is intractable to go through all possible values of the latent code $\mathbf{z}$.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Attention? Attention!</title>
      <link>https://lilianweng.github.io/posts/2018-06-24-attention/</link>
      <pubDate>Sun, 24 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-06-24-attention/</guid>
      <description>&lt;!-- Attention has been a fairly popular concept and a useful tool in the deep learning community in recent years. In this post, we are gonna look into how attention was invented, and various attention mechanisms and models, such as transformer and SNAIL. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2018-10-28: Add &lt;a href=&#34;#pointer-network&#34;&gt;Pointer Network&lt;/a&gt; and the &lt;a href=&#34;https://github.com/lilianweng/transformer-tensorflow&#34;&gt;link&lt;/a&gt; to my implementation of Transformer.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2018-11-06: Add a &lt;a href=&#34;https://github.com/lilianweng/transformer-tensorflow&#34;&gt;link&lt;/a&gt; to the implementation of Transformer model.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2018-11-18: Add &lt;a href=&#34;#neural-turing-machines&#34;&gt;Neural Turing Machines&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2019-07-18: Correct the mistake on using the term &amp;ldquo;self-attention&amp;rdquo; when introducing the &lt;a href=&#34;https://arxiv.org/abs/1502.03044&#34;&gt;show-attention-tell&lt;/a&gt; paper; moved it to &lt;a href=&#34;#self-attention&#34;&gt;Self-Attention&lt;/a&gt; section.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-04-07: A follow-up post on improved Transformer models is &lt;a href=&#34;https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/&#34;&gt;here&lt;/a&gt;.]&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
