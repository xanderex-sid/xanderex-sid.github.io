<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Meta-Learning on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/meta-learning/</link>
    <description>Recent content in Meta-Learning on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Jan 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/meta-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Curriculum for Reinforcement Learning</title>
      <link>https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/</link>
      <pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/</guid>
      <description>&lt;!-- A curriculum is an efficient tool for humans to progressively learn from simple concepts to hard problems. It breaks down complex knowledge by providing a sequence of learning steps of increasing difficulty. In this post, we will examine how the idea of curriculum can help reinforcement learning models learn to solve complicated tasks. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-02-03: mentioning &lt;a href=&#34;#pcg&#34;&gt;PCG&lt;/a&gt; in the &amp;ldquo;Task-Specific Curriculum&amp;rdquo; section.&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-02-04: Add a new &lt;a href=&#34;#curriculum-through-distillation&#34;&gt;&amp;ldquo;curriculum through distillation&amp;rdquo;&lt;/a&gt; section.&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Meta Reinforcement Learning</title>
      <link>https://lilianweng.github.io/posts/2019-06-23-meta-rl/</link>
      <pubDate>Sun, 23 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2019-06-23-meta-rl/</guid>
      <description>&lt;!-- Meta-RL is meta-learning on reinforcement learning tasks. After trained over a distribution of tasks, the agent is able to solve a new task by developing a new RL algorithm with its internal activity dynamics. This post starts with the origin of meta-RL and then dives into three key components of meta-RL. --&gt;
&lt;p&gt;In my earlier post on &lt;a href=&#34;https://lilianweng.github.io/posts/2018-11-30-meta-learning/&#34;&gt;meta-learning&lt;/a&gt;, the problem is mainly defined in the context of few-shot classification. Here I would like to explore more into cases when we try to &amp;ldquo;meta-learn&amp;rdquo; &lt;a href=&#34;https://lilianweng.github.io/posts/2018-02-19-rl-overview/&#34;&gt;Reinforcement Learning (RL)&lt;/a&gt; tasks by developing an agent that can solve unseen tasks fast and efficiently.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Domain Randomization for Sim2Real Transfer</title>
      <link>https://lilianweng.github.io/posts/2019-05-05-domain-randomization/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2019-05-05-domain-randomization/</guid>
      <description>&lt;!-- If a model or policy is mainly trained in a simulator but expected to work on a real robot, it would surely face the sim2real gap. *Domain Randomization* (DR) is a simple but powerful idea of closing this gap by randomizing properties of the training environment. --&gt;
&lt;p&gt;In Robotics, one of the hardest problems is how to make your model transfer to the real world. Due to the sample inefficiency of deep RL algorithms and the cost of data collection on real robots, we often need to train models in a simulator which theoretically provides an infinite amount of data. However, the reality gap between the simulator and the physical world often leads to failure when working with physical robots. The gap is triggered by an inconsistency between physical parameters (i.e. friction, kp, damping, mass, density) and, more fatally, the incorrect physical modeling (i.e. collision between soft surfaces).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Meta-Learning: Learning to Learn Fast</title>
      <link>https://lilianweng.github.io/posts/2018-11-30-meta-learning/</link>
      <pubDate>Fri, 30 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-11-30-meta-learning/</guid>
      <description>&lt;!-- Meta-learning, also known as &#34;learning to learn&#34;, intends to design models that can learn new skills or adapt to new environments rapidly with a few training examples. There are three common approaches: 1) learn an efficient distance metric (metric-based); 2) use (recurrent) network with external or internal memory (model-based); 3) optimize the model parameters explicitly for fast learning (optimization-based). --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2019-10-01: thanks to Tianhao, we have this post translated in &lt;a href=&#34;https://wei-tianhao.github.io/blog/2019/09/17/meta-learning.html&#34;&gt;Chinese&lt;/a&gt;!]&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
