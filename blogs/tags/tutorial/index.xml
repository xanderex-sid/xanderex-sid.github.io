<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Tutorial on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/tutorial/</link>
    <description>Recent content in Tutorial on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 05 May 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/tutorial/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Implementing Deep Reinforcement Learning Models with Tensorflow &#43; OpenAI Gym</title>
      <link>https://lilianweng.github.io/posts/2018-05-05-drl-implementation/</link>
      <pubDate>Sat, 05 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-05-05-drl-implementation/</guid>
      <description>&lt;!-- Let&#39;s see how to implement a number of classic deep reinforcement learning models in code. --&gt;
&lt;p&gt;The full implementation is available in &lt;a href=&#34;https://github.com/lilianweng/deep-reinforcement-learning-gym&#34;&gt;lilianweng/deep-reinforcement-learning-gym&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the previous two posts, I have introduced the algorithms of many deep reinforcement learning models. Now it is the time to get our hands dirty and practice how to implement the models in the wild. The implementation is gonna be built in Tensorflow and OpenAI &lt;a href=&#34;https://github.com/openai/gym&#34;&gt;gym&lt;/a&gt; environment. The full version of the code in this tutorial is available in &lt;a href=&#34;https://github.com/lilianweng/deep-reinforcement-learning-gym&#34;&gt;[lilian/deep-reinforcement-learning-gym]&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Predict Stock Prices Using RNN: Part 2</title>
      <link>https://lilianweng.github.io/posts/2017-07-22-stock-rnn-part-2/</link>
      <pubDate>Sat, 22 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2017-07-22-stock-rnn-part-2/</guid>
      <description>&lt;!-- This post is a continued tutorial for how to build a recurrent neural network using Tensorflow to predict stock market prices. Part 2 attempts to predict prices of multiple stocks using embeddings. The full working code is available in [lilianweng/stock-rnn](https://github.com/lilianweng/stock-rnn). --&gt;
&lt;p&gt;In the Part 2 tutorial, I would like to continue the topic on stock price prediction and to endow the recurrent neural network that I have built in &lt;a href=&#34;https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/&#34;&gt;Part 1&lt;/a&gt; with the capability of responding to multiple stocks. In order to distinguish the patterns associated with different price sequences, I use the stock symbol embedding vectors as part of the input.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Predict Stock Prices Using RNN: Part 1</title>
      <link>https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/</link>
      <pubDate>Sat, 08 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/</guid>
      <description>&lt;!-- This post is a tutorial for how to build a recurrent neural network using Tensorflow to predict stock market prices. Part 1 focuses on the prediction of S&amp;P 500 index. The full working code is available in [lilianweng/stock-rnn](https://github.com/lilianweng/stock-rnn). --&gt;
&lt;p&gt;This is a tutorial for how to build a recurrent neural network using Tensorflow to predict stock market prices. The full working code is available in &lt;a href=&#34;https://github.com/lilianweng/stock-rnn&#34;&gt;github.com/lilianweng/stock-rnn&lt;/a&gt;. If you don&amp;rsquo;t know what is recurrent neural network or LSTM cell, feel free to check &lt;a href=&#34;https://lilianweng.github.io/posts/2017-06-21-overview/#recurrent-neural-network&#34;&gt;my previous post&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>An Overview of Deep Learning for Curious People</title>
      <link>https://lilianweng.github.io/posts/2017-06-21-overview/</link>
      <pubDate>Wed, 21 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2017-06-21-overview/</guid>
      <description>&lt;!-- Starting earlier this year, I grew a strong curiosity of deep learning and spent some time reading about this field. To document what Iâ€™ve learned and to provide some interesting pointers to people with similar interests, I wrote this overview of deep learning models and their applications. --&gt;
&lt;p&gt;&lt;span style=&#34;color: #aaaaaa;&#34;&gt;(The post was originated from my talk for &lt;a href=&#34;http://wimlds.org/chapters/about-bay-area/&#34;&gt;WiMLDS x Fintech meetup&lt;/a&gt; hosted by &lt;a href=&#34;www.affirm.com&#34;&gt;Affirm&lt;/a&gt;.)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I believe many of you have watched or heard of the &lt;a href=&#34;https://youtu.be/vFr3K2DORc8&#34;&gt;games&lt;/a&gt; between AlphaGo and professional Go player &lt;a href=&#34;https://en.wikipedia.org/wiki/Lee_Sedol&#34;&gt;Lee Sedol&lt;/a&gt; in 2016. Lee has the highest rank of nine dan and many world championships. No doubt, he is one of the best Go players in the world, but he &lt;a href=&#34;https://www.scientificamerican.com/article/how-the-computer-beat-the-go-master/&#34;&gt;lost by 1-4&lt;/a&gt; in this series versus AlphaGo. Before this, Go was considered to be an intractable game for computers to master, as its simple rules lay out an exponential number of variations in the board positions, many more than what in Chess. This event surely highlighted 2016 as a big year for AI. Because of AlphaGo, much attention has been attracted to the progress of AI.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
