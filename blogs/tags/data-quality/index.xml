<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Data-Quality on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/data-quality/</link>
    <description>Recent content in Data-Quality on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 05 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/data-quality/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Thinking about High-Quality Human Data</title>
      <link>https://lilianweng.github.io/posts/2024-02-05-human-data-quality/</link>
      <pubDate>Mon, 05 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2024-02-05-human-data-quality/</guid>
      <description>&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Special thank you to &lt;a href=&#34;https://scholar.google.com/citations?user=FRBObOwAAAAJ&amp;amp;hl=en&#34;&gt;Ian Kivlichan&lt;/a&gt; for many useful pointers (E.g. the 100+ year old Nature paper &amp;ldquo;Vox populi&amp;rdquo;) and nice feedback. üôè ]&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;High-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation, such as classification task or &lt;a href=&#34;https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#rl-fine-tuning-with-human-preferences&#34;&gt;RLHF&lt;/a&gt; labeling (which can be constructed as classification format) for LLM alignment training. Lots of ML techniques in the post can help with data quality, but fundamentally human data collection involves attention to details and careful execution. The community knows the value of high quality data, but somehow we have this subtle impression that ‚ÄúEveryone wants to do the model work, not the data work‚Äù (&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3411764.3445518&#34;&gt;Sambasivan et al. 2021&lt;/a&gt;).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Learning with not Enough Data Part 3: Data Generation</title>
      <link>https://lilianweng.github.io/posts/2022-04-15-data-gen/</link>
      <pubDate>Fri, 15 Apr 2022 15:10:30 -0700</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2022-04-15-data-gen/</guid>
      <description>&lt;p&gt;Here comes the Part 3 on learning with not enough data (Previous: &lt;a href=&#34;https://lilianweng.github.io/posts/2021-12-05-semi-supervised/&#34;&gt;Part 1&lt;/a&gt; and &lt;a href=&#34;https://lilianweng.github.io/posts/2022-02-20-active-learning/&#34;&gt;Part 2&lt;/a&gt;). Let‚Äôs consider two approaches for generating synthetic data for training.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Augmented data&lt;/strong&gt;. Given a set of existing training samples, we can apply a variety of augmentation, distortion and transformation to derive new data points without losing the key attributes. We have covered a bunch of augmentation methods on text and images in a &lt;a href=&#34;https://lilianweng.github.io/posts/2021-05-31-contrastive/&#34;&gt;previous post&lt;/a&gt; on contrastive learning. For the sake of post completeness, I &lt;em&gt;duplicate&lt;/em&gt; the section on data augmentation here with some edits.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;New data&lt;/strong&gt;. Given few or even no data points, we can rely on powerful pretrained models to generate a number of &lt;em&gt;new&lt;/em&gt; data points. This is especially true in recent years given the fast progress in large pretrained &lt;a href=&#34;https://lilianweng.github.io/posts/2019-01-31-lm/&#34;&gt;language models (LM)&lt;/a&gt;. Few shot prompting is shown to be effective for LM to learn within context without extra training.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;data-augmentation&#34;&gt;Data Augmentation&lt;/h1&gt;
&lt;p&gt;The goal of data augmentation is to modify the input format (e.g. text wording, visual appearance) while the semantic meaning stays unchanged.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
