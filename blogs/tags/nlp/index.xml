<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Nlp on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/nlp/</link>
    <description>Recent content in Nlp on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 07 Jul 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Extrinsic Hallucinations in LLMs</title>
      <link>https://lilianweng.github.io/posts/2024-07-07-hallucination/</link>
      <pubDate>Sun, 07 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2024-07-07-hallucination/</guid>
      <description>&lt;p&gt;Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and &lt;strong&gt;not grounded&lt;/strong&gt; by either the provided context or world knowledge.&lt;/p&gt;
&lt;p&gt;There are two types of hallucination:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In-context hallucination: The model output should be consistent with the source content in context.&lt;/li&gt;
&lt;li&gt;Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Adversarial Attacks on LLMs</title>
      <link>https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/</link>
      <pubDate>Wed, 25 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/</guid>
      <description>&lt;p&gt;The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via &lt;a href=&#34;https://openai.com/research/learning-to-summarize-with-human-feedback&#34;&gt;RLHF&lt;/a&gt;). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.&lt;/p&gt;
&lt;p&gt;A large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on &lt;a href=&#34;https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/&#34;&gt;Controllable Text Generation&lt;/a&gt; is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LLM Powered Autonomous Agents</title>
      <link>https://lilianweng.github.io/posts/2023-06-23-agent/</link>
      <pubDate>Fri, 23 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2023-06-23-agent/</guid>
      <description>&lt;p&gt;Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as &lt;a href=&#34;https://github.com/Significant-Gravitas/Auto-GPT&#34;&gt;AutoGPT&lt;/a&gt;, &lt;a href=&#34;https://github.com/AntonOsika/gpt-engineer&#34;&gt;GPT-Engineer&lt;/a&gt; and &lt;a href=&#34;https://github.com/yoheinakajima/babyagi&#34;&gt;BabyAGI&lt;/a&gt;, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.&lt;/p&gt;
&lt;h1 id=&#34;agent-system-overview&#34;&gt;Agent System Overview&lt;/h1&gt;
&lt;p&gt;In a LLM-powered autonomous agent system, LLM functions as the agent&amp;rsquo;s brain, complemented by several key components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Planning&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.&lt;/li&gt;
&lt;li&gt;Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Short-term memory: I would consider all the in-context learning (See &lt;a href=&#34;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/&#34;&gt;Prompt Engineering&lt;/a&gt;) as utilizing short-term memory of the model to learn.&lt;/li&gt;
&lt;li&gt;Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tool use&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
	&lt;img src=&#34;agent-overview.png&#34; style=&#34;width: 100%;&#34;  /&gt;
	&lt;figcaption&gt;Overview of a LLM-powered autonomous agent system.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h1 id=&#34;component-one-planning&#34;&gt;Component One: Planning&lt;/h1&gt;
&lt;p&gt;A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Prompt Engineering</title>
      <link>https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/</link>
      <pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Prompt Engineering&lt;/strong&gt;, also known as &lt;strong&gt;In-Context Prompting&lt;/strong&gt;, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes &lt;em&gt;without&lt;/em&gt; updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.&lt;/p&gt;
&lt;p&gt;This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my &lt;a href=&#34;https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/&#34;&gt;previous post&lt;/a&gt; on controllable text generation.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Reducing Toxicity in Language Models</title>
      <link>https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/</link>
      <pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/</guid>
      <description>&lt;!-- Toxicity prevents us from safely deploying powerful pretrained language models for real-world applications. To reduce toxicity in language models, in this post, we will delve into three aspects of the problem: training dataset collection, toxic content detection and model detoxification. --&gt;
&lt;p&gt;Large pretrained &lt;a href=&#34;https://lilianweng.github.io/posts/2019-01-31-lm/&#34;&gt;language models&lt;/a&gt; are trained over a sizable collection of online data. They unavoidably acquire certain toxic behavior and biases from the Internet. Pretrained language models are very powerful and have shown great success in many NLP tasks. However, to safely deploy them for practical real-world applications demands a strong safety control over the model generation process.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Controllable Neural Text Generation</title>
      <link>https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/</guid>
      <description>&lt;!-- The modern language model with SOTA results on many NLP tasks is trained on large scale free text on the Internet. It is challenging to steer such a model to generate content with desired attributes. Although still not perfect, there are several approaches for controllable text generation, such as guided or learned decoding strategy, smart prompt design, or fine-tuning the model with various methods. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2021-02-01: Updated to version 2.0 with several work added and many typos fixed.]&lt;/span&gt;
&lt;br /&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2021-05-26: Add P-tuning and Prompt Tuning in the &lt;a href=&#34;#gradient-based-search&#34;&gt;&amp;ldquo;prompt design&amp;rdquo;&lt;/a&gt; section.]&lt;/span&gt;
&lt;br /&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2021-09-19: Add &lt;a href=&#34;##unlikelihood-training&#34;&gt;&amp;ldquo;unlikelihood training&amp;rdquo;&lt;/a&gt;.]&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How to Build an Open-Domain Question Answering System?</title>
      <link>https://lilianweng.github.io/posts/2020-10-29-odqa/</link>
      <pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2020-10-29-odqa/</guid>
      <description>&lt;!-- A model that is capable of answering any question with regard to factual knowledge can enable many useful applications. This post delves into how we can build an Open-Domain Question Answering (ODQA) system, assuming we have access to a powerful pretrained language model. Both closed-book and open-book approachs are discussed. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-11-12: add &lt;a href=&#34;#openai-api-example&#34;&gt;an example&lt;/a&gt; on closed-book factual QA using OpenAI API (beta).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A model that can answer any question with regard to factual knowledge can lead to many useful and practical applications, such as working as a chatbot or an AI assistant🤖. In this post, we will review several common approaches for building such an open-domain question answering system.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Generalized Language Models</title>
      <link>https://lilianweng.github.io/posts/2019-01-31-lm/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2019-01-31-lm/</guid>
      <description>&lt;!-- As a follow up of word embedding post, we will discuss the models on learning contextualized word vectors, as well as the new trend in large unsupervised pre-trained language models which have achieved amazing SOTA results on a variety of language tasks. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2019-02-14: add &lt;a href=&#34;#ulmfit&#34;&gt;ULMFiT&lt;/a&gt; and &lt;a href=&#34;#gpt-2&#34;&gt;GPT-2&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-02-29: add &lt;a href=&#34;#albert&#34;&gt;ALBERT&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-10-25: add &lt;a href=&#34;#roberta&#34;&gt;RoBERTa&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-12-13: add &lt;a href=&#34;#t5&#34;&gt;T5&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-12-30: add &lt;a href=&#34;#gpt-3&#34;&gt;GPT-3&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2021-11-13: add &lt;a href=&#34;#xlnet&#34;&gt;XLNet&lt;/a&gt;, &lt;a href=&#34;#bart&#34;&gt;BART&lt;/a&gt; and &lt;a href=&#34;#electra&#34;&gt;ELECTRA&lt;/a&gt;; Also updated the &lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt; section.]&lt;/span&gt;&lt;/p&gt;
&lt;br /&gt;
&lt;figure&gt;
	&lt;img src=&#34;elmo-and-bert.png&#34; style=&#34;width: 60%;&#34;  /&gt;
	&lt;figcaption&gt;I guess they are Elmo &amp; Bert? (Image source: &lt;a href=&#34;https://www.youtube.com/watch?v=l5einDQ-Ttc&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We have seen amazing progress in NLP in 2018. Large-scale pre-trained language modes like &lt;a href=&#34;https://blog.openai.com/language-unsupervised/&#34;&gt;OpenAI GPT&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;BERT&lt;/a&gt; have achieved great performance on a variety of language tasks using generic model architectures. The idea is similar to how ImageNet classification pre-training helps many vision tasks (*). Even better than vision classification pre-training, this simple and powerful approach in NLP does not require labeled data for pre-training, allowing us to experiment with increased training scale, up to our very limit.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Learning Word Embedding</title>
      <link>https://lilianweng.github.io/posts/2017-10-15-word-embedding/</link>
      <pubDate>Sun, 15 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2017-10-15-word-embedding/</guid>
      <description>&lt;!-- Word embedding is a dense representation of words in the form of numeric vectors. It can be learned using a variety of language models. The word embedding representation is able to reveal many hidden relationships between words. For example, vector(&#34;cat&#34;) - vector(&#34;kitten&#34;) is similar to vector(&#34;dog&#34;) - vector(&#34;puppy&#34;). This post introduces several models for learning word embedding and how their loss functions are designed for the purpose. --&gt;
&lt;p&gt;Human vocabulary comes in free text. In order to make a machine learning model understand and process the natural language, we need to transform the free-text words into numeric values. One of the simplest transformation approaches is to do a one-hot encoding in which each distinct word stands for one dimension of the resulting vector and a binary value indicates whether the word presents (1) or not (0).&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
