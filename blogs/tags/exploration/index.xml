<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Exploration on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/exploration/</link>
    <description>Recent content in Exploration on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 07 Jun 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/exploration/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Exploration Strategies in Deep Reinforcement Learning</title>
      <link>https://lilianweng.github.io/posts/2020-06-07-exploration-drl/</link>
      <pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2020-06-07-exploration-drl/</guid>
      <description>&lt;!-- Exploitation versus exploration is a critical topic in reinforcement learning. This post introduces several common approaches for better exploration in Deep RL. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-06-17: Add &lt;a href=&#34;#exploration-via-disagreement&#34;&gt;&amp;ldquo;exploration via disagreement&amp;rdquo;&lt;/a&gt; in the &amp;ldquo;Forward Dynamics&amp;rdquo; &lt;a href=&#34;#forward-dynamics&#34;&gt;section&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/&#34;&gt;Exploitation versus exploration&lt;/a&gt; is a critical topic in Reinforcement Learning. We&amp;rsquo;d like the RL agent to find the best solution as fast as possible. However, in the meantime, committing to solutions too quickly without enough exploration sounds pretty bad, as it could lead to local minima or total failure. Modern &lt;a href=&#34;https://lilianweng.github.io/posts/2018-02-19-rl-overview/&#34;&gt;RL&lt;/a&gt; &lt;a href=&#34;https://lilianweng.github.io/posts/2018-04-08-policy-gradient/&#34;&gt;algorithms&lt;/a&gt; that optimize for the best returns can achieve good exploitation quite efficiently, while exploration remains more like an open topic.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Multi-Armed Bandit Problem and Its Solutions</title>
      <link>https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/</link>
      <pubDate>Tue, 23 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/</guid>
      <description>&lt;!-- The multi-armed bandit problem is a class example to demonstrate the exploration versus exploitation dilemma. This post introduces the bandit problem and how to solve it using different exploration strategies. --&gt;
&lt;p&gt;The algorithms are implemented for Bernoulli bandit in &lt;a href=&#34;http://github.com/lilianweng/multi-armed-bandit&#34;&gt;lilianweng/multi-armed-bandit&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;exploitation-vs-exploration&#34;&gt;Exploitation vs Exploration&lt;/h1&gt;
&lt;p&gt;The exploration vs exploitation dilemma exists in many aspects of our life. Say, your favorite restaurant is right around the corner. If you go there every day, you would be confident of what you will get, but miss the chances of discovering an even better option. If you try new places all the time, very likely you are gonna have to eat unpleasant food from time to time. Similarly, online advisors try to balance between the known most attractive ads and the new ads that might be even more successful.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
