<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Rnn on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/rnn/</link>
    <description>Recent content in Rnn on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 24 Jun 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/rnn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Attention? Attention!</title>
      <link>https://lilianweng.github.io/posts/2018-06-24-attention/</link>
      <pubDate>Sun, 24 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-06-24-attention/</guid>
      <description>&lt;!-- Attention has been a fairly popular concept and a useful tool in the deep learning community in recent years. In this post, we are gonna look into how attention was invented, and various attention mechanisms and models, such as transformer and SNAIL. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2018-10-28: Add &lt;a href=&#34;#pointer-network&#34;&gt;Pointer Network&lt;/a&gt; and the &lt;a href=&#34;https://github.com/lilianweng/transformer-tensorflow&#34;&gt;link&lt;/a&gt; to my implementation of Transformer.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2018-11-06: Add a &lt;a href=&#34;https://github.com/lilianweng/transformer-tensorflow&#34;&gt;link&lt;/a&gt; to the implementation of Transformer model.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2018-11-18: Add &lt;a href=&#34;#neural-turing-machines&#34;&gt;Neural Turing Machines&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2019-07-18: Correct the mistake on using the term &amp;ldquo;self-attention&amp;rdquo; when introducing the &lt;a href=&#34;https://arxiv.org/abs/1502.03044&#34;&gt;show-attention-tell&lt;/a&gt; paper; moved it to &lt;a href=&#34;#self-attention&#34;&gt;Self-Attention&lt;/a&gt; section.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-04-07: A follow-up post on improved Transformer models is &lt;a href=&#34;https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/&#34;&gt;here&lt;/a&gt;.]&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Predict Stock Prices Using RNN: Part 2</title>
      <link>https://lilianweng.github.io/posts/2017-07-22-stock-rnn-part-2/</link>
      <pubDate>Sat, 22 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2017-07-22-stock-rnn-part-2/</guid>
      <description>&lt;!-- This post is a continued tutorial for how to build a recurrent neural network using Tensorflow to predict stock market prices. Part 2 attempts to predict prices of multiple stocks using embeddings. The full working code is available in [lilianweng/stock-rnn](https://github.com/lilianweng/stock-rnn). --&gt;
&lt;p&gt;In the Part 2 tutorial, I would like to continue the topic on stock price prediction and to endow the recurrent neural network that I have built in &lt;a href=&#34;https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/&#34;&gt;Part 1&lt;/a&gt; with the capability of responding to multiple stocks. In order to distinguish the patterns associated with different price sequences, I use the stock symbol embedding vectors as part of the input.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Predict Stock Prices Using RNN: Part 1</title>
      <link>https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/</link>
      <pubDate>Sat, 08 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/</guid>
      <description>&lt;!-- This post is a tutorial for how to build a recurrent neural network using Tensorflow to predict stock market prices. Part 1 focuses on the prediction of S&amp;P 500 index. The full working code is available in [lilianweng/stock-rnn](https://github.com/lilianweng/stock-rnn). --&gt;
&lt;p&gt;This is a tutorial for how to build a recurrent neural network using Tensorflow to predict stock market prices. The full working code is available in &lt;a href=&#34;https://github.com/lilianweng/stock-rnn&#34;&gt;github.com/lilianweng/stock-rnn&lt;/a&gt;. If you don&amp;rsquo;t know what is recurrent neural network or LSTM cell, feel free to check &lt;a href=&#34;https://lilianweng.github.io/posts/2017-06-21-overview/#recurrent-neural-network&#34;&gt;my previous post&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
