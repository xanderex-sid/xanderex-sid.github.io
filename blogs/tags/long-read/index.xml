<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Long-Read on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/long-read/</link>
    <description>Recent content in Long-Read on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/long-read/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Why We Think</title>
      <link>https://lilianweng.github.io/posts/2025-05-01-thinking/</link>
      <pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2025-05-01-thinking/</guid>
      <description>&lt;p&gt;&lt;span class=&#34;update&#34;&gt;Special thanks to &lt;a href=&#34;https://scholar.google.com/citations?user=itSa94cAAAAJ&amp;amp;hl=en&#34;&gt;John Schulman&lt;/a&gt; for a lot of super valuable feedback and direct edits on this post.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Test time compute (&lt;a href=&#34;https://arxiv.org/abs/1603.08983&#34;&gt;Graves et al. 2016&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1705.04146&#34;&gt;Ling, et al. 2017&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2110.14168&#34;&gt;Cobbe et al. 2021&lt;/a&gt;) and Chain-of-thought (CoT) (&lt;a href=&#34;https://arxiv.org/abs/2201.11903&#34;&gt;Wei et al. 2022&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2112.00114&#34;&gt;Nye et al. 2021&lt;/a&gt;), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. &amp;ldquo;thinking time&amp;rdquo;) and why it helps.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Reward Hacking in Reinforcement Learning</title>
      <link>https://lilianweng.github.io/posts/2024-11-28-reward-hacking/</link>
      <pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2024-11-28-reward-hacking/</guid>
      <description>&lt;p&gt;Reward hacking occurs when a &lt;a href=&#34;(https://lilianweng.github.io/posts/2018-02-19-rl-overview/)&#34;&gt;reinforcement learning (RL)&lt;/a&gt; agent &lt;a href=&#34;https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#exploitation-vs-exploration&#34;&gt;exploits&lt;/a&gt; flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function.&lt;/p&gt;
&lt;p&gt;With the rise of &lt;a href=&#34;https://lilianweng.github.io/posts/2019-01-31-lm/&#34;&gt;language models&lt;/a&gt; generalizing to a broad spectrum of tasks and RLHF becomes a de facto method for alignment training, reward hacking in RL training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user&amp;rsquo;s preference, are pretty concerning and are likely one of the major blockers for real-world deployment of more autonomous use cases of AI models.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Transformer Family Version 2.0</title>
      <link>https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/</link>
      <pubDate>Fri, 27 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/</guid>
      <description>&lt;p&gt;Many new Transformer architecture improvements have been proposed since my last post on &lt;a href=&#34;https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/&#34;&gt;&lt;ins&gt;&amp;ldquo;The Transformer Family&amp;rdquo;&lt;/ins&gt;&lt;/a&gt; about three years ago. Here I did a big refactoring and enrichment of that 2020 post &amp;mdash; restructure the hierarchy of sections and improve many sections with more recent papers. Version 2.0 is a superset of the old version, about twice the length.&lt;/p&gt;
&lt;h1 id=&#34;notations&#34;&gt;Notations&lt;/h1&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Symbol&lt;/th&gt;
          &lt;th&gt;Meaning&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;$d$&lt;/td&gt;
          &lt;td&gt;The model size / hidden state dimension / positional encoding size.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$h$&lt;/td&gt;
          &lt;td&gt;The number of heads in multi-head attention layer.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$L$&lt;/td&gt;
          &lt;td&gt;The segment length of input sequence.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$N$&lt;/td&gt;
          &lt;td&gt;The total number of attention layers in the model; not considering MoE.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{X} \in \mathbb{R}^{L \times d}$&lt;/td&gt;
          &lt;td&gt;The input sequence where each element has been mapped into an embedding vector of shape $d$, same as the model size.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{W}^k \in \mathbb{R}^{d \times d_k}$&lt;/td&gt;
          &lt;td&gt;The key weight matrix.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{W}^q \in \mathbb{R}^{d \times d_k}$&lt;/td&gt;
          &lt;td&gt;The query weight matrix.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{W}^v \in \mathbb{R}^{d \times d_v}$&lt;/td&gt;
          &lt;td&gt;The value weight matrix. Often we have $d_k = d_v = d$.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{W}^k_i, \mathbf{W}^q_i \in \mathbb{R}^{d \times d_k/h}; \mathbf{W}^v_i \in \mathbb{R}^{d \times d_v/h}$&lt;/td&gt;
          &lt;td&gt;The weight matrices per head.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{W}^o \in \mathbb{R}^{d_v \times d}$&lt;/td&gt;
          &lt;td&gt;The output weight matrix.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{Q} = \mathbf{X}\mathbf{W}^q \in \mathbb{R}^{L \times d_k}$&lt;/td&gt;
          &lt;td&gt;The query embedding inputs.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{K} = \mathbf{X}\mathbf{W}^k \in \mathbb{R}^{L \times d_k}$&lt;/td&gt;
          &lt;td&gt;The key embedding inputs.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{V} = \mathbf{X}\mathbf{W}^v \in \mathbb{R}^{L \times d_v}$&lt;/td&gt;
          &lt;td&gt;The value embedding inputs.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{q}_i, \mathbf{k}_i \in \mathbb{R}^{d_k}, \mathbf{v}_i \in \mathbb{R}^{d_v}$&lt;/td&gt;
          &lt;td&gt;Row vectors in query, key, value matrices, $\mathbf{Q}$, $\mathbf{K}$ and $\mathbf{V}$.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$S_i$&lt;/td&gt;
          &lt;td&gt;A collection of key positions for the $i$-th query $\mathbf{q}_i$ to attend to.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{A} \in \mathbb{R}^{L \times L}$&lt;/td&gt;
          &lt;td&gt;The self-attention matrix between a input sequence of lenght $L$ and itself. $\mathbf{A} = \text{softmax}(\mathbf{Q}\mathbf{K}^\top / \sqrt{d_k})$.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$a_{ij} \in \mathbf{A}$&lt;/td&gt;
          &lt;td&gt;The scalar attention score between query $\mathbf{q}_i$ and key $\mathbf{k}_j$.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{P} \in \mathbb{R}^{L \times d}$&lt;/td&gt;
          &lt;td&gt;position encoding matrix, where the $i$-th row $\mathbf{p}_i$ is the positional encoding for input $\mathbf{x}_i$.&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;transformer-basics&#34;&gt;Transformer Basics&lt;/h1&gt;
&lt;p&gt;The &lt;strong&gt;Transformer&lt;/strong&gt; (which will be referred to as &amp;ldquo;vanilla Transformer&amp;rdquo; to distinguish it from other enhanced versions; &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Vaswani, et al., 2017&lt;/a&gt;) model has an encoder-decoder architecture, as commonly used in many &lt;a href=&#34;https://lilianweng.github.io/posts/2018-06-24-attention/#born-for-translation&#34;&gt;NMT&lt;/a&gt; models. Later simplified Transformer was shown to achieve great performance in language modeling tasks, like in encoder-only &lt;a href=&#34;https://lilianweng.github.io/posts/2019-01-31-lm/#bert&#34;&gt;BERT&lt;/a&gt; or decoder-only &lt;a href=&#34;https://lilianweng.github.io/posts/2019-01-31-lm/#openai-gpt&#34;&gt;GPT&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Contrastive Representation Learning</title>
      <link>https://lilianweng.github.io/posts/2021-05-31-contrastive/</link>
      <pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2021-05-31-contrastive/</guid>
      <description>&lt;!-- The main idea of contrastive learning is to learn representations such that similar samples stay close to each other, while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised data and has been shown to achieve good performance on a variety of vision and language tasks. --&gt;
&lt;p&gt;The goal of contrastive representation learning is to learn such an embedding space in which similar sample pairs stay close to each other while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised settings. When working with unsupervised data, contrastive learning is one of the most powerful approaches in &lt;a href=&#34;https://lilianweng.github.io/posts/2019-11-10-self-supervised/&#34;&gt;self-supervised learning&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Controllable Neural Text Generation</title>
      <link>https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/</guid>
      <description>&lt;!-- The modern language model with SOTA results on many NLP tasks is trained on large scale free text on the Internet. It is challenging to steer such a model to generate content with desired attributes. Although still not perfect, there are several approaches for controllable text generation, such as guided or learned decoding strategy, smart prompt design, or fine-tuning the model with various methods. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2021-02-01: Updated to version 2.0 with several work added and many typos fixed.]&lt;/span&gt;
&lt;br /&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2021-05-26: Add P-tuning and Prompt Tuning in the &lt;a href=&#34;#gradient-based-search&#34;&gt;&amp;ldquo;prompt design&amp;rdquo;&lt;/a&gt; section.]&lt;/span&gt;
&lt;br /&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2021-09-19: Add &lt;a href=&#34;##unlikelihood-training&#34;&gt;&amp;ldquo;unlikelihood training&amp;rdquo;&lt;/a&gt;.]&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Exploration Strategies in Deep Reinforcement Learning</title>
      <link>https://lilianweng.github.io/posts/2020-06-07-exploration-drl/</link>
      <pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2020-06-07-exploration-drl/</guid>
      <description>&lt;!-- Exploitation versus exploration is a critical topic in reinforcement learning. This post introduces several common approaches for better exploration in Deep RL. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-06-17: Add &lt;a href=&#34;#exploration-via-disagreement&#34;&gt;&amp;ldquo;exploration via disagreement&amp;rdquo;&lt;/a&gt; in the &amp;ldquo;Forward Dynamics&amp;rdquo; &lt;a href=&#34;#forward-dynamics&#34;&gt;section&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/&#34;&gt;Exploitation versus exploration&lt;/a&gt; is a critical topic in Reinforcement Learning. We&amp;rsquo;d like the RL agent to find the best solution as fast as possible. However, in the meantime, committing to solutions too quickly without enough exploration sounds pretty bad, as it could lead to local minima or total failure. Modern &lt;a href=&#34;https://lilianweng.github.io/posts/2018-02-19-rl-overview/&#34;&gt;RL&lt;/a&gt; &lt;a href=&#34;https://lilianweng.github.io/posts/2018-04-08-policy-gradient/&#34;&gt;algorithms&lt;/a&gt; that optimize for the best returns can achieve good exploitation quite efficiently, while exploration remains more like an open topic.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Self-Supervised Representation Learning</title>
      <link>https://lilianweng.github.io/posts/2019-11-10-self-supervised/</link>
      <pubDate>Sun, 10 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2019-11-10-self-supervised/</guid>
      <description>&lt;!-- Self-supervised learning opens up a huge opportunity for better utilizing unlabelled data, while learning in a supervised learning manner. This post covers many interesting ideas of self-supervised learning tasks on images, videos, and control problems. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-01-09: add a new section on &lt;a href=&#34;#contrastive-predictive-coding&#34;&gt;Contrastive Predictive Coding&lt;/a&gt;].&lt;/span&gt;
&lt;br/&gt;
&lt;del&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-04-13: add a &amp;ldquo;Momentum Contrast&amp;rdquo; section on MoCo, SimCLR and CURL.]&lt;/span&gt;&lt;/del&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-07-08: add a &lt;a href=&#34;#bisimulation&#34;&gt;&amp;ldquo;Bisimulation&amp;rdquo;&lt;/a&gt; section on DeepMDP and DBC.]&lt;/span&gt;
&lt;br/&gt;
&lt;del&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-09-12: add &lt;a href=&#34;https://lilianweng.github.io/posts/2021-05-31-contrastive/#moco--moco-v2&#34;&gt;MoCo V2&lt;/a&gt; and &lt;a href=&#34;https://lilianweng.github.io/posts/2021-05-31-contrastive/#byol&#34;&gt;BYOL&lt;/a&gt; in the &amp;ldquo;Momentum Contrast&amp;rdquo; section.]&lt;/span&gt;&lt;/del&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2021-05-31: remove section on &amp;ldquo;Momentum Contrast&amp;rdquo; and add a pointer to a full post on &lt;a href=&#34;https://lilianweng.github.io/posts/2021-05-31-contrastive/&#34;&gt;&amp;ldquo;Contrastive Representation Learning&amp;rdquo;&lt;/a&gt;]&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Generalized Language Models</title>
      <link>https://lilianweng.github.io/posts/2019-01-31-lm/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2019-01-31-lm/</guid>
      <description>&lt;!-- As a follow up of word embedding post, we will discuss the models on learning contextualized word vectors, as well as the new trend in large unsupervised pre-trained language models which have achieved amazing SOTA results on a variety of language tasks. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2019-02-14: add &lt;a href=&#34;#ulmfit&#34;&gt;ULMFiT&lt;/a&gt; and &lt;a href=&#34;#gpt-2&#34;&gt;GPT-2&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-02-29: add &lt;a href=&#34;#albert&#34;&gt;ALBERT&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-10-25: add &lt;a href=&#34;#roberta&#34;&gt;RoBERTa&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-12-13: add &lt;a href=&#34;#t5&#34;&gt;T5&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-12-30: add &lt;a href=&#34;#gpt-3&#34;&gt;GPT-3&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2021-11-13: add &lt;a href=&#34;#xlnet&#34;&gt;XLNet&lt;/a&gt;, &lt;a href=&#34;#bart&#34;&gt;BART&lt;/a&gt; and &lt;a href=&#34;#electra&#34;&gt;ELECTRA&lt;/a&gt;; Also updated the &lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt; section.]&lt;/span&gt;&lt;/p&gt;
&lt;br /&gt;
&lt;figure&gt;
	&lt;img src=&#34;elmo-and-bert.png&#34; style=&#34;width: 60%;&#34;  /&gt;
	&lt;figcaption&gt;I guess they are Elmo &amp; Bert? (Image source: &lt;a href=&#34;https://www.youtube.com/watch?v=l5einDQ-Ttc&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We have seen amazing progress in NLP in 2018. Large-scale pre-trained language modes like &lt;a href=&#34;https://blog.openai.com/language-unsupervised/&#34;&gt;OpenAI GPT&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;BERT&lt;/a&gt; have achieved great performance on a variety of language tasks using generic model architectures. The idea is similar to how ImageNet classification pre-training helps many vision tasks (*). Even better than vision classification pre-training, this simple and powerful approach in NLP does not require labeled data for pre-training, allowing us to experiment with increased training scale, up to our very limit.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Meta-Learning: Learning to Learn Fast</title>
      <link>https://lilianweng.github.io/posts/2018-11-30-meta-learning/</link>
      <pubDate>Fri, 30 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-11-30-meta-learning/</guid>
      <description>&lt;!-- Meta-learning, also known as &#34;learning to learn&#34;, intends to design models that can learn new skills or adapt to new environments rapidly with a few training examples. There are three common approaches: 1) learn an efficient distance metric (metric-based); 2) use (recurrent) network with external or internal memory (model-based); 3) optimize the model parameters explicitly for fast learning (optimization-based). --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2019-10-01: thanks to Tianhao, we have this post translated in &lt;a href=&#34;https://wei-tianhao.github.io/blog/2019/09/17/meta-learning.html&#34;&gt;Chinese&lt;/a&gt;!]&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Policy Gradient Algorithms</title>
      <link>https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</link>
      <pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</guid>
      <description>&lt;!-- Abstract: In this post, we are going to look deep into policy gradient, why it works, and many new policy gradient algorithms proposed in recent years: vanilla policy gradient, actor-critic, off-policy actor-critic, A3C, A2C, DPG, DDPG, D4PG, MADDPG, TRPO, PPO, ACER, ACTKR, SAC, TD3 &amp; SVPG. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2018-06-30: add two new policy gradient methods, &lt;a href=&#34;#sac&#34;&gt;SAC&lt;/a&gt; and &lt;a href=&#34;#d4pg&#34;&gt;D4PG&lt;/a&gt;.]&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2018-09-30: add a new policy gradient method, &lt;a href=&#34;#td3&#34;&gt;TD3&lt;/a&gt;.]&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2019-02-09: add &lt;a href=&#34;#sac-with-automatically-adjusted-temperature&#34;&gt;SAC with automatically adjusted temperature&lt;/a&gt;].&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2019-06-26: Thanks to Chanseok, we have a version of this post in &lt;a href=&#34;https://talkingaboutme.tistory.com/entry/RL-Policy-Gradient-Algorithms&#34;&gt;Korean&lt;/a&gt;].&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2019-09-12: add a new policy gradient method &lt;a href=&#34;#svpg&#34;&gt;SVPG&lt;/a&gt;.]&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2019-12-22: add a new policy gradient method &lt;a href=&#34;#impala&#34;&gt;IMPALA&lt;/a&gt;.]&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-10-15: add a new policy gradient method &lt;a href=&#34;#ppg&#34;&gt;PPG&lt;/a&gt; &amp;amp; some new discussion in &lt;a href=&#34;#ppo&#34;&gt;PPO&lt;/a&gt;.]&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2021-09-19: Thanks to Wenhao &amp;amp; 爱吃猫的鱼, we have this post in &lt;a href=&#34;https://tomaxent.com/2019/04/14/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%96%B9%E6%B3%95/&#34;&gt;Chinese1&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://paperexplained.cn/articles/article/detail/31/&#34;&gt;Chinese2&lt;/a&gt;].&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A (Long) Peek into Reinforcement Learning</title>
      <link>https://lilianweng.github.io/posts/2018-02-19-rl-overview/</link>
      <pubDate>Mon, 19 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-02-19-rl-overview/</guid>
      <description>&lt;!-- In this post, we are gonna briefly go over the field of Reinforcement Learning (RL), from fundamental concepts to classic algorithms. Hopefully, this review is helpful enough so that newbies would not get lost in specialized terms and jargons while starting. [WARNING] This is a long read. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-09-03: Updated the algorithm of &lt;a href=&#34;#sarsa-on-policy-td-control&#34;&gt;SARSA&lt;/a&gt; and &lt;a href=&#34;#q-learning-off-policy-td-control&#34;&gt;Q-learning&lt;/a&gt; so that the difference is more pronounced.&lt;/span&gt;
&lt;br /&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2021-09-19: Thanks to 爱吃猫的鱼, we have this post in &lt;a href=&#34;https://paperexplained.cn/articles/article/detail/33/&#34;&gt;Chinese&lt;/a&gt;].&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>From GAN to WGAN</title>
      <link>https://lilianweng.github.io/posts/2017-08-20-gan/</link>
      <pubDate>Sun, 20 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2017-08-20-gan/</guid>
      <description>&lt;!-- This post explains the maths behind a generative adversarial network (GAN) model and why it is hard to be trained. Wasserstein GAN is intended to improve GANs&#39; training by adopting a smooth metric for measuring the distance between two probability distributions. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2018-09-30: thanks to Yoonju, we have this post translated in &lt;a href=&#34;https://github.com/yjucho1/articles/blob/master/fromGANtoWGAN/readme.md&#34;&gt;Korean&lt;/a&gt;!]&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2019-04-18: this post is also available on &lt;a href=&#34;https://arxiv.org/abs/1904.08994&#34;&gt;arXiv&lt;/a&gt;.]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1406.2661.pdf&#34;&gt;Generative adversarial network&lt;/a&gt; (GAN) has shown great results in many generative tasks to replicate the real-world rich content such as images, human language, and music. It is inspired by game theory: two models, a generator and a critic, are competing with each other while making each other stronger at the same time. However, it is rather challenging to train a GAN model, as people are facing issues like training instability or failure to converge.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
