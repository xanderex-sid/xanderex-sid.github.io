<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Representation-Learning on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/representation-learning/</link>
    <description>Recent content in Representation-Learning on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 31 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/representation-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Contrastive Representation Learning</title>
      <link>https://lilianweng.github.io/posts/2021-05-31-contrastive/</link>
      <pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2021-05-31-contrastive/</guid>
      <description>&lt;!-- The main idea of contrastive learning is to learn representations such that similar samples stay close to each other, while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised data and has been shown to achieve good performance on a variety of vision and language tasks. --&gt;
&lt;p&gt;The goal of contrastive representation learning is to learn such an embedding space in which similar sample pairs stay close to each other while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised settings. When working with unsupervised data, contrastive learning is one of the most powerful approaches in &lt;a href=&#34;https://lilianweng.github.io/posts/2019-11-10-self-supervised/&#34;&gt;self-supervised learning&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Self-Supervised Representation Learning</title>
      <link>https://lilianweng.github.io/posts/2019-11-10-self-supervised/</link>
      <pubDate>Sun, 10 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2019-11-10-self-supervised/</guid>
      <description>&lt;!-- Self-supervised learning opens up a huge opportunity for better utilizing unlabelled data, while learning in a supervised learning manner. This post covers many interesting ideas of self-supervised learning tasks on images, videos, and control problems. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-01-09: add a new section on &lt;a href=&#34;#contrastive-predictive-coding&#34;&gt;Contrastive Predictive Coding&lt;/a&gt;].&lt;/span&gt;
&lt;br/&gt;
&lt;del&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-04-13: add a &amp;ldquo;Momentum Contrast&amp;rdquo; section on MoCo, SimCLR and CURL.]&lt;/span&gt;&lt;/del&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-07-08: add a &lt;a href=&#34;#bisimulation&#34;&gt;&amp;ldquo;Bisimulation&amp;rdquo;&lt;/a&gt; section on DeepMDP and DBC.]&lt;/span&gt;
&lt;br/&gt;
&lt;del&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-09-12: add &lt;a href=&#34;https://lilianweng.github.io/posts/2021-05-31-contrastive/#moco--moco-v2&#34;&gt;MoCo V2&lt;/a&gt; and &lt;a href=&#34;https://lilianweng.github.io/posts/2021-05-31-contrastive/#byol&#34;&gt;BYOL&lt;/a&gt; in the &amp;ldquo;Momentum Contrast&amp;rdquo; section.]&lt;/span&gt;&lt;/del&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2021-05-31: remove section on &amp;ldquo;Momentum Contrast&amp;rdquo; and add a pointer to a full post on &lt;a href=&#34;https://lilianweng.github.io/posts/2021-05-31-contrastive/&#34;&gt;&amp;ldquo;Contrastive Representation Learning&amp;rdquo;&lt;/a&gt;]&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
