<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Generative-Model on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/generative-model/</link>
    <description>Recent content in Generative-Model on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Apr 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/generative-model/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Diffusion Models for Video Generation</title>
      <link>https://lilianweng.github.io/posts/2024-04-12-diffusion-video/</link>
      <pubDate>Fri, 12 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2024-04-12-diffusion-video/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2021-07-11-diffusion-models/&#34;&gt;Diffusion models&lt;/a&gt; have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task&amp;mdash;using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.&lt;/li&gt;
&lt;li&gt;In comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;br/&gt;&lt;b&gt;
ðŸ¥‘ Required Pre-read: Please make sure you have read the previous blog on &lt;a href=&#34;https://lilianweng.github.io/posts/2021-07-11-diffusion-models/&#34;&gt;&amp;ldquo;What are Diffusion Models?&amp;rdquo;&lt;/a&gt; for image generation before continue here.
&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>What are Diffusion Models?</title>
      <link>https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</link>
      <pubDate>Sun, 11 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</guid>
      <description>&lt;!-- Diffusion models are a new type of generative models that are flexible enough to learn any arbitrarily complex data distribution while tractable to analytically evaluate the distribution. It has been shown recently that diffusion models can generate high-quality images and the performance is competitive to SOTA GAN. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2021-09-19: Highly recommend this blog post on &lt;a href=&#34;https://yang-song.github.io/blog/2021/score/&#34;&gt;score-based generative modeling&lt;/a&gt; by Yang Song (author of several key papers in the references)].&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2022-08-27: Added &lt;a href=&#34;#classifier-free-guidance&#34;&gt;classifier-free guidance&lt;/a&gt;, &lt;a href=&#34;#glide&#34;&gt;GLIDE&lt;/a&gt;, &lt;a href=&#34;#unclip&#34;&gt;unCLIP&lt;/a&gt; and &lt;a href=&#34;#imagen&#34;&gt;Imagen&lt;/a&gt;.&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2022-08-31: Added &lt;a href=&#34;#ldm&#34;&gt;latent diffusion model&lt;/a&gt;.&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2024-04-13: Added &lt;a href=&#34;#prog-distll&#34;&gt;progressive distillation&lt;/a&gt;, &lt;a href=&#34;#consistency&#34;&gt;consistency models&lt;/a&gt;, and the &lt;a href=&#34;#model-architecture&#34;&gt;Model Architecture section&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Curriculum for Reinforcement Learning</title>
      <link>https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/</link>
      <pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/</guid>
      <description>&lt;!-- A curriculum is an efficient tool for humans to progressively learn from simple concepts to hard problems. It breaks down complex knowledge by providing a sequence of learning steps of increasing difficulty. In this post, we will examine how the idea of curriculum can help reinforcement learning models learn to solve complicated tasks. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-02-03: mentioning &lt;a href=&#34;#pcg&#34;&gt;PCG&lt;/a&gt; in the &amp;ldquo;Task-Specific Curriculum&amp;rdquo; section.&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-02-04: Add a new &lt;a href=&#34;#curriculum-through-distillation&#34;&gt;&amp;ldquo;curriculum through distillation&amp;rdquo;&lt;/a&gt; section.&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Self-Supervised Representation Learning</title>
      <link>https://lilianweng.github.io/posts/2019-11-10-self-supervised/</link>
      <pubDate>Sun, 10 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2019-11-10-self-supervised/</guid>
      <description>&lt;!-- Self-supervised learning opens up a huge opportunity for better utilizing unlabelled data, while learning in a supervised learning manner. This post covers many interesting ideas of self-supervised learning tasks on images, videos, and control problems. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-01-09: add a new section on &lt;a href=&#34;#contrastive-predictive-coding&#34;&gt;Contrastive Predictive Coding&lt;/a&gt;].&lt;/span&gt;
&lt;br/&gt;
&lt;del&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-04-13: add a &amp;ldquo;Momentum Contrast&amp;rdquo; section on MoCo, SimCLR and CURL.]&lt;/span&gt;&lt;/del&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2020-07-08: add a &lt;a href=&#34;#bisimulation&#34;&gt;&amp;ldquo;Bisimulation&amp;rdquo;&lt;/a&gt; section on DeepMDP and DBC.]&lt;/span&gt;
&lt;br/&gt;
&lt;del&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2020-09-12: add &lt;a href=&#34;https://lilianweng.github.io/posts/2021-05-31-contrastive/#moco--moco-v2&#34;&gt;MoCo V2&lt;/a&gt; and &lt;a href=&#34;https://lilianweng.github.io/posts/2021-05-31-contrastive/#byol&#34;&gt;BYOL&lt;/a&gt; in the &amp;ldquo;Momentum Contrast&amp;rdquo; section.]&lt;/span&gt;&lt;/del&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2021-05-31: remove section on &amp;ldquo;Momentum Contrast&amp;rdquo; and add a pointer to a full post on &lt;a href=&#34;https://lilianweng.github.io/posts/2021-05-31-contrastive/&#34;&gt;&amp;ldquo;Contrastive Representation Learning&amp;rdquo;&lt;/a&gt;]&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Flow-based Deep Generative Models</title>
      <link>https://lilianweng.github.io/posts/2018-10-13-flow-models/</link>
      <pubDate>Sat, 13 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-10-13-flow-models/</guid>
      <description>&lt;!-- In this post, we are looking into the third type of generative models: flow-based generative models. Different from GAN and VAE, they explicitly learn the probability density function of the input data. --&gt;
&lt;p&gt;So far, I&amp;rsquo;ve written about two types of generative models, &lt;a href=&#34;https://lilianweng.github.io/posts/2017-08-20-gan/&#34;&gt;GAN&lt;/a&gt; and &lt;a href=&#34;https://lilianweng.github.io/posts/2018-08-12-vae/&#34;&gt;VAE&lt;/a&gt;. Neither of them explicitly learns the probability density function of real data, $p(\mathbf{x})$ (where $\mathbf{x} \in \mathcal{D}$) &amp;mdash; because it is really hard! Taking the generative model with latent variables as an example, $p(\mathbf{x}) = \int p(\mathbf{x}\vert\mathbf{z})p(\mathbf{z})d\mathbf{z}$ can hardly be calculated as it is intractable to go through all possible values of the latent code $\mathbf{z}$.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>From Autoencoder to Beta-VAE</title>
      <link>https://lilianweng.github.io/posts/2018-08-12-vae/</link>
      <pubDate>Sun, 12 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-08-12-vae/</guid>
      <description>&lt;!-- Autocoders are a family of neural network models aiming to learn compressed latent variables of high-dimensional data. Starting from the basic autocoder model, this post reviews several variations, including denoising, sparse, and contractive autoencoders, and then Variational Autoencoder (VAE) and its modification beta-VAE. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2019-07-18: add a section on &lt;a href=&#34;#vq-vae-and-vq-vae-2&#34;&gt;VQ-VAE &amp;amp; VQ-VAE-2&lt;/a&gt;.]&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2019-07-26: add a section on &lt;a href=&#34;#td-vae&#34;&gt;TD-VAE&lt;/a&gt;.]&lt;/span&gt;
&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Autocoder is invented to reconstruct high-dimensional data using a neural network model with a narrow bottleneck layer in the middle (oops, this is probably not true for &lt;a href=&#34;#vae-variational-autoencoder&#34;&gt;Variational Autoencoder&lt;/a&gt;, and we will investigate it in details in later sections). A nice byproduct is dimension reduction: the bottleneck layer captures a compressed latent encoding. Such a low-dimensional representation can be used as en embedding vector in various applications (i.e. search), help data compression, or reveal the underlying data generative factors.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>From GAN to WGAN</title>
      <link>https://lilianweng.github.io/posts/2017-08-20-gan/</link>
      <pubDate>Sun, 20 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2017-08-20-gan/</guid>
      <description>&lt;!-- This post explains the maths behind a generative adversarial network (GAN) model and why it is hard to be trained. Wasserstein GAN is intended to improve GANs&#39; training by adopting a smooth metric for measuring the distance between two probability distributions. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2018-09-30: thanks to Yoonju, we have this post translated in &lt;a href=&#34;https://github.com/yjucho1/articles/blob/master/fromGANtoWGAN/readme.md&#34;&gt;Korean&lt;/a&gt;!]&lt;/span&gt;
&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2019-04-18: this post is also available on &lt;a href=&#34;https://arxiv.org/abs/1904.08994&#34;&gt;arXiv&lt;/a&gt;.]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1406.2661.pdf&#34;&gt;Generative adversarial network&lt;/a&gt; (GAN) has shown great results in many generative tasks to replicate the real-world rich content such as images, human language, and music. It is inspired by game theory: two models, a generator and a critic, are competing with each other while making each other stronger at the same time. However, it is rather challenging to train a GAN model, as people are facing issues like training instability or failure to converge.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
